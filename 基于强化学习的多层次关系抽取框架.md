#  <center>A Hierarchical Framework for Relation Extraction with Reinforcement Learning</center>

---
关系抽取工作是从非结构化文本数据中提取出一句话中两个实体之间的关系，可用三元组（es，r，et）来表示。目前关系抽取工作存在两个问题，一个是关系抽取任务一般在实体识别任务之后，将关系抽取和命名实体识别看做是两个独立的任务，实体识别和关系抽取两个任务之间的相互关联没有很好地被挖掘，针对这一问题，现在有很多结合联合学习的研究；另一个问题是联合抽取方法在抽取一对多关系，也就是一个实体在多个关系中、或一对实体拥有多种关系，这类问题上还找不到很好的解决方案。针对这两个问题，论文提出了一种基于分层强化学习的方法，把关系抽取作为high-level的任务，实体识别作为low-level任务完成关系抽取工作。

整个方法的流程如下图所示。I. 首先从头到尾扫描整个句子序列；II. 在high-level的过程中，在某一位置侦测到了一个关系的指示器，这时触发low-level任务进行相关实体的识别；III. Low-level过程从头到尾扫描句子，识别到两个和该关系相关的实体；IV. 回到刚才关系指示器的位置，继续high-level的过程，识别下一个关系。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937693381165368200418.png)

### 相关工作
---
目前进行关系抽取的工作有两大类方法：流水线方法（pipelined approaches）和联合学习方法（joint learning methods）。流水线方法先做实体抽取再进行关系抽取，模型搭建较为灵活，但会有错误传播问题。联合学习方法起初是基于图结构的方法，使用贪心查找策略，限制了性能，其他基于机器学习的方法依赖于大量的手工特征和领域知识。近年来出现了很多基于神经网络模型的联合学习方法，通过共享实体抽取和关系抽取模型的参数，或基于新的标注策略等，在一定程度上改进了实体和关系联合抽取的方法。目前有很多信息抽取的研究引入了强化学习，用于对远程监督数据进行去噪，和对数据集进行扩充。

---

### 方法
---
论文的方法在基于分层强化学习的方法上，以关系抽取作为高层级任务，实体识别作为低层级任务进行研究，将任务分割成有层级关系的两个强化学习过程，可用于识别有多个关系类型的实体对。

首先定义了关系指示器（relation indicator）的概念。关系指示器是在有足够的信息被提及来识别一个语义关系时，此时扫描到句子中的位置。与关系触发器（relation trigger）不同，关系指示器可以是动词、名词、介词，或者是其他诸如逗号、句号的符号。关系指示器在这个模型中很重要，整个任务被分为关系指示器的识别、和实体抽取。整个抽取过程是：顺序扫描一个句子，agent在特定位置预测一个关系类型，这里关系的预测不需要实体的标注，不像关系分类需要识别一个实体对之间的关系。如果在一个时间步内没有充分的证据识别出一个关系，agent给出NR-no relation。如果一个关系指示器被触发，agent进入子任务中，进行两个实体的识别作为关系的论证。当两个实体都被识别出来，子任务结束，继续扫描剩下的句子，寻找其他关系。整个过程的示意图如下图所示。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937694292206966553757.png)

两个框架的抽取策略如图所示，左边是关系检测的策略，右边是实体抽取的策略

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937694301566972823745.png)


下面分别介绍两个层级的强化学习策略。High-level强化学习基于option，是关系类型的集合。state表示high-level的强化学习在时间步t的状态，由当前隐层状态ht、关系类型的向量vtr（可学习的参数），和上一个状态st-1表示。Fh是一个非线性函数，MLP（多层感知机）实现。st-1是h（high-level）或l（low-level）的一种，由agent在high中的option或low中的action进行采样决定。隐层状态ht由当前词向量经过BiLSTM实现。策略和奖励的计算如下所示。当ot=NR时，agent在下一个时间步变换为新的high状态；否则low-level的policy将执行实体抽取。当前ot的子任务完成后，inter option的状态才会改变，可能会持续多个时间步。最后的奖励r由计算句子级的抽取性能得到，FB是由对句子S中关系抽取的精度和召回率计算得到的带权调和平均值。β是一个超参。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937694309366999566315.png)

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937708212111407285545.png)

下面介绍Low-level的学习策略。High-level的policy预测了非NR关系类型，low-level的policy就抽取相关的实体。High-level的ot作为low-level的额外输入信息。动作空间是实体标签空间，S代表头实体、T代表尾实体、O代表与关系类型ot‘无关的实体，N代表非实体词，解决了overlapping关系的问题。BI表示该词在实体中的位置信息。State和关系检测中的policy相似，low的内部option的state，stl表示为当前词wt的隐层状态ht、实体标志向量vte，可由at-1学习得到，前一状态st-1和上下文向量ct‘，使用和前面公式相似的状态表示。ht是由Bi-LSTM得到的隐层状态，g和f都是由MLP多层感知机实现的非线性函数。St-1可能由high-level或low-level得到。实体抽取的策略是给定内部状态stl，和导致当前子任务的high-level option ot’ 关系类型，给出一个动作的分布。λ是对无实体标签的偏倚权重，α表示对非实体词的奖励要少一些,避免了把所有词预测为非实体。所有动作结束后有一个最终奖励，如果所有tag都预测正确，agent得到的reward是+1，否则是-1。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695498089075895531.png)

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695504329089408856.png)

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695513689101380672.png)

High-level和low-level的累积奖励由下式求得。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695521489118123242.png)

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695526169129879905.png)

对其求梯度:

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695533969146149893.png)

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937695543329157121709.png)

最终整个过程的训练算法是：

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937698034653538758380.png)

---

### 实验
---
实验采用纽约时报的语料作为数据集，由远程监督构建的数据集，含有噪声。语料有两个版本，一个由原始语料通过Freebase知识库的关系类型进行调整得到，取名为NYT10，一个是较小的数据集，它的测试集全部是手工标注的，取名为NYT11。

论文对数据集进行了处理，移除了训练集中测试集不包含的关系类型，移除了不包含关系的语句。为公平起见，所有baseline也使用处理后的数据集进行评估，处理后数据集如表所示。这里对数据集的处理相当于移除了数据中的负例，但是这样的处理是否适用于实际应用文中并没有讨论。每个数据集上都使用训练集的0.5%作为验证集。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937698042453546500951.png)

实验采用micro-作为评价指标，下面是baseline方法的介绍：FCM是流水线方法，MultiR和CoType是基于特征的方法，后三组是神经网络的方法。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937699318535783525860.png)

主要的实验结果如表：这里解释了CopeR主要用于提取重叠关系，所以在NYT11上效果不好，因为NYT11上几乎没有重叠关系数据；SPTree使用了很多语言学信息，所以效果相对较好，但论文提出的模型在两个数据集上的F值都为最高，证明模型的鲁棒性很好。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937699327895804497676.png)

之后为验证模型在重叠关系提取上的性能，构建了两个数据集。重叠关系分为两类：type1：两个三元组只有一个实体相同；type2：两个三元组共享两个实体。NYT11-plus是经过人工标注，由nyt11训练集产生，大部分是type1类型的重叠关系，NYT10-sub是NYT10的子集，大部分是type2类型的重叠关系。实验结果如下：

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937709489753657539699.png)

NYT10-sub的结果显示baseline方法不适合在有噪声的数据集抽取type2类型重叠关系，而提出的HRL模型和表2相比结果没有恶化，在预测的准确率上还有提升；NYT11-plus结果显示本文的模型和其他方法相比F1有显著提高，证明该方法能够更准确地提取关系。Sptree只匹配了一个关系类型没考虑重叠关系，所以准确率很高；Tagging方法结果不好，因为他只给实体分配一个标签；copyR方法依赖于噪声数据集中的标注。总之可以看出提取重叠关系还是很有困难，本文提出的方法在两个数据集上表现都比其他方法好。
为了证明将实体抽取整合到关系抽取任务的效果，来看两个层级策略之间的相互作用。这部分只关注关系检测的表现，就是如果关系类型预测正确了就认为预测是正确的，关系预测由high-level的策略得到。下表中结果显示提出的强化学习方法相比其他方法，对多关系的抽取效果更好，如果把low-level的实体抽取去掉，表现在NYT11数据集上有细微变化，这个数据集上没有重叠关系，而NYT11-plus上具有重叠关系，所以看到去除实体抽取部分结果差别很大，这也证明了提出的这个方法可以很好地抓住实体抽取和关系抽取之间的依赖关系，可以增强它们的相互作用。

![iamge](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937701934660389745489.png)

最后看一个例子，下图中是模型对两句具有重叠关系的语句进行三元组提取的过程。可以看出，关系检测在一个位置有足够的信息时就会被触发，并且重叠关系也能够不受彼此影响分别提取出来。

![image](https://github.com/weikang-wang/-paper/blob/master/image/5-10/6368937701942460397488059.png)


### 结论

